{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follower network\n",
    "We start from the Twitter follower network constructed for the paper [Right and left, partisanship predicts (asymmetric) vulnerability to misinformation](http://doi.org/10.37016/mr-2020-55). The following data files are available at https://doi.org/10.7910/DVN/6CZHH5:\n",
    "* `anonymized-friends.json` \n",
    "* `measures.tab`\n",
    "\n",
    "Briefly, this network was constructed as follows:\n",
    "* We collected all tweets containing links (URLs) from a 10% random sample of public posts between June 1 and June 30, 2017, through the Twitter Decahose. \n",
    "* We selected all accounts that shared at least ten links from a set of news sources with known political valence (Bakshy et al., 2015). \n",
    "* We further selected those who shared at least one link from a source labeled as low-quality (https://github.com/BigMcLargeHuge/opensources). \n",
    "* We excluded likely bot accounts according to the BotometerLite classifier (Yang et al., 2020).\n",
    "\n",
    "We keep the nodes with both partisanship and misinformation attributes, then we take the core of the network with approximately 10k nodes, and finally remove a random sample of edges to preserve the original average in/out-degree (number of friends/followers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import importlib\n",
    "import bot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../EmpiricalNet_followers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File has 3 columns: ID \\t partisanship \\t misinformation \\n\n",
    "partisanship = {}\n",
    "misinformation = {}\n",
    "with open(path + \"measures.tab\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\")\n",
    "    next(rd) # skip header row\n",
    "    for row in rd:\n",
    "        partisanship[int(row[0])] = row[1]\n",
    "        misinformation[int(row[0])] = row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'anonymized-friends.json') as fp:\n",
    "    adjlist = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directed network follower -> friend\n",
    "for s in adjlist:\n",
    "    n = int(s)\n",
    "    if n in partisanship and n in misinformation:\n",
    "        G.add_node(n, party=partisanship[n], misinfo=misinformation[n]) \n",
    "        for f in adjlist[s]:\n",
    "            G.add_edge(n,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58048 nodes and 10499218 edges initially, with average number of friends 180.87131339581038\n",
      "15056 nodes and 4327448 edges after filtering\n"
     ]
    }
   ],
   "source": [
    "average_friends = G.number_of_edges() / G.number_of_nodes()\n",
    "print(\"{} nodes and {} edges initially, with average number of friends {}\".format(G.number_of_nodes(), G.number_of_edges(), average_friends))\n",
    "friends = nx.subgraph(G, partisanship.keys())\n",
    "print(\"{} nodes and {} edges after filtering\".format(friends.number_of_nodes(), friends.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94-core has 10006 nodes, 4144687 edges\n"
     ]
    }
   ],
   "source": [
    "# k-core decomposition until ~ 10k nodes in core\n",
    "core_number = nx.core_number(friends)\n",
    "nodes = friends.number_of_nodes()\n",
    "k = 0\n",
    "while nodes > 10000:\n",
    "    k_core = nx.k_core(friends, k, core_number)\n",
    "    nodes = k_core.number_of_nodes()\n",
    "    k += 10\n",
    "while nodes < 10000:\n",
    "    k_core = nx.k_core(friends, k, core_number)\n",
    "    nodes = k_core.number_of_nodes()\n",
    "    k -= 1\n",
    "print(\"{}-core has {} nodes, {} edges\".format(k, k_core.number_of_nodes(), k_core.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94-core after edge-sampling has 10006 nodes, 1809798 edges, and average number of friends 180.8712772336598\n"
     ]
    }
   ],
   "source": [
    "# the network is super dense, so let us delete a random sample of edges\n",
    "# we can set the initial average in/out-degree (average_friends) as a target \n",
    "friends_core = k_core.copy()\n",
    "edges_to_keep = int(friends_core.number_of_nodes() * average_friends)\n",
    "edges_to_delete = friends_core.number_of_edges() - edges_to_keep\n",
    "deleted_edges = random.sample(friends_core.edges(), edges_to_delete)\n",
    "friends_core.remove_edges_from(deleted_edges)\n",
    "print(\"{}-core after edge-sampling has {} nodes, {} edges, and average number of friends {}\".format(k, friends_core.number_of_nodes(), friends_core.number_of_edges(), friends_core.number_of_edges() / friends_core.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(friends_core, path + 'follower_network.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading human network...\n",
      "Generating bot network...\n",
      "Merging human and bot networks...\n",
      "Humans following bots...\n",
      "time_steps =  0 , q =  200\n",
      "time_steps =  1 , q =  0.4801671070272221\n",
      "time_steps =  2 , q =  0.4931137924415558\n",
      "average quality for follower network: 0.49038907607976123\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "importlib.reload(bot_model)\n",
    "follower_net = bot_model.init_net(False, verbose=True, human_network = path + 'follower_network.gml', beta=0.01, gamma=0.001)\n",
    "avg_quality = bot_model.simulation(False, network=follower_net, verbose=True, mu=0.5, phi=1, alpha=15)\n",
    "print('average quality for follower network:', avg_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments \n",
    "# params:    alpha, beta, gamma, phi, flooding\n",
    "# targeting: hubs, partisans, conservatives, misinfo spreaders, core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
