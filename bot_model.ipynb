{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4VdiGhQd57n1"
   },
   "source": [
    "Reproducing model in paper with Xiaodan: https://www.overleaf.com/project/5c375cf8f540a47e999968db\n",
    "\n",
    "Notes:\n",
    "* Need Python 3.6 or later; eg: `module load python/3.6.6`\n",
    "* remember link direction is following, opposite of info spread!\n",
    ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:21:42.454586Z",
     "start_time": "2020-05-25T15:21:40.763176Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "byMDogYTqly4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import random\n",
    "import numpy\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import fcntl\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:21:47.830099Z",
     "start_time": "2020-05-25T15:21:47.824017Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xFWqYkRp5uHr"
   },
   "outputs": [],
   "source": [
    "# parameters and utility globals\n",
    "\n",
    "n_humans = 1000 # 10k for paper\n",
    "beta = 0.1 # bots/humans ratio; 0.1 for paper\n",
    "p = 0.5 # for network clustering; 0.5 for paper\n",
    "k_out = 3 # average no. friends within humans & bots; 3 for paper\n",
    "alpha = 15 # depth of feed; 15 for paper\n",
    "mu = 0.75 # average prob of new meme vs retweet; 0.75 for paper or draw from empirical distribution\n",
    "# phi = 1 \n",
    "# gamma = 0.1 \n",
    "epsilon = 0.01 # threshold used to check for steady-state convergence\n",
    "n_runs = 10 # number of simulations to average results\n",
    "cvsfile = 'results.csv' # to save results for plotting\n",
    "\n",
    "phis = [1, 5, 10] # bot deception >= 1: meme fitness higher than quality \n",
    "gammas = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0] # infiltration: probability that a human follows each bot\n",
    "\n",
    "# if called with gamma as a command line params\n",
    "if len(sys.argv) == 2:\n",
    "  gamma = float(sys.argv[1])\n",
    "  assert(0 <= gamma <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T14:23:23.899932Z",
     "start_time": "2020-05-11T14:23:23.893328Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "H_X9MDFyst06"
   },
   "outputs": [],
   "source": [
    "# create a network with random-walk growth model\n",
    "\n",
    "def random_walk_network(net_size):\n",
    "  if net_size <= k_out + 1: # if super small just return a clique\n",
    "    return nx.complete_graph(net_size, create_using=nx.DiGraph())\n",
    "  G = nx.complete_graph(k_out, create_using=nx.DiGraph()) \n",
    "  for n in range(k_out, net_size):\n",
    "    target = random.choice(list(G.nodes()))\n",
    "    friends = [target]\n",
    "    n_random_friends = 0\n",
    "    for _ in range(k_out - 1):\n",
    "      if random.random() < p:\n",
    "        n_random_friends += 1\n",
    "    friends.extend(random.sample(list(G.successors(target)), n_random_friends))\n",
    "    friends.extend(random.sample(list(G.nodes()), k_out - 1 - n_random_friends))\n",
    "    G.add_node(n)\n",
    "    for f in friends:\n",
    "      G.add_edge(n, f)\n",
    "  return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T14:23:23.910237Z",
     "start_time": "2020-05-11T14:23:23.903472Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gKL2qHkSPafv"
   },
   "outputs": [],
   "source": [
    "# sample a bunch of objects from a list without replacement \n",
    "# and with given weights (to be used as probabilities), which can be zero\n",
    "# NB: cannot use random_choices, which samples with replacement\n",
    "#     nor numpy.random.choice, which can only use non-zero probabilities\n",
    "\n",
    "def sample_with_prob_without_replacement(elements, sample_size, weights): \n",
    "  \n",
    "  # first remove the elements with zero prob, normalize rest\n",
    "  assert(len(elements) == len(weights))\n",
    "  total = 0\n",
    "  non_zeros = []\n",
    "  probs = []\n",
    "  zeros = []\n",
    "  for i in range(len(elements)):\n",
    "    if weights[i] > 0:\n",
    "      non_zeros.append(elements[i])\n",
    "      probs.append(weights[i])\n",
    "      total += weights[i]\n",
    "    else: \n",
    "      zeros.append(elements[i])\n",
    "  probs = [w/total for w in probs]\n",
    "\n",
    "  # if we have enough elements with non-zero probabilities, sample from those\n",
    "  if sample_size <= len(non_zeros):\n",
    "    return numpy.random.choice(non_zeros, p=probs, size=sample_size, replace=False)\n",
    "  else:\n",
    "    # if we need more, take all the elements with non-zero probability\n",
    "    # plus a random sample of the elements with zero probability\n",
    "    return non_zeros + random.sample(zeros, sample_size - len(non_zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:50:12.547955Z",
     "start_time": "2020-05-11T16:50:12.539467Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vvlFGXktrf0q"
   },
   "outputs": [],
   "source": [
    "# create network of humans and bots\n",
    "# preferential_targeting is a flag; if False, random targeting\n",
    "\n",
    "def init_net(preferential_targeting):\n",
    "\n",
    "  # humans\n",
    "  H = random_walk_network(n_humans)\n",
    "  for h in H.nodes:\n",
    "    H.nodes[h]['bot'] = False\n",
    "\n",
    "  # bots\n",
    "  n_bots = int(n_humans * beta) \n",
    "  B = random_walk_network(n_bots)\n",
    "  for b in B.nodes:\n",
    "    B.nodes[b]['bot'] = True\n",
    "\n",
    "  # merge and add feed\n",
    "  # feed is array of (quality, fitness) tuples\n",
    "  G = nx.disjoint_union(H, B)\n",
    "  assert(G.number_of_nodes() == n_humans + n_bots)\n",
    "  humans = []\n",
    "  bots = []\n",
    "  for n in G.nodes:\n",
    "    G.nodes[n]['feed'] = []\n",
    "    if G.nodes[n]['bot']:\n",
    "      bots.append(n)\n",
    "    else:\n",
    "      humans.append(n)\n",
    "\n",
    "  # humans follow bots\n",
    "  w = [G.in_degree(h) for h in humans]\n",
    "  for b in bots:\n",
    "    n_followers = 0\n",
    "    for _ in humans:\n",
    "      if random.random() < gamma:\n",
    "        n_followers += 1\n",
    "    if preferential_targeting:\n",
    "      followers = sample_with_prob_without_replacement(humans, n_followers, w)\n",
    "    else:\n",
    "      followers = random.sample(humans, n_followers)\n",
    "    for f in followers:\n",
    "      G.add_edge(f, b)\n",
    "\n",
    "  return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:50:16.911412Z",
     "start_time": "2020-05-11T16:50:16.907092Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "e9hFkeerYIIl"
   },
   "outputs": [],
   "source": [
    "# return (quality, fitness) tuple depending on bot flag\n",
    "# using https://en.wikipedia.org/wiki/Inverse_transform_sampling\n",
    "\n",
    "def get_meme(bot_flag):\n",
    "\n",
    "  if bot_flag:\n",
    "    exponent = 1 + (1 / phi)\n",
    "  else:\n",
    "    exponent = 1 + phi\n",
    "  u = random.random()\n",
    "  fitness = 1 - (1 - u)**(1 / exponent)\n",
    "  if bot_flag:\n",
    "    quality = 0\n",
    "  else:\n",
    "    quality = fitness\n",
    "  return (quality, fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:25:07.039549Z",
     "start_time": "2020-05-11T16:25:07.036080Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "2hKncWumMjYB"
   },
   "outputs": [],
   "source": [
    "# count the number of forgotten memes as a function of in_degree (followers)\n",
    "# using a global variable that needs to be reset\n",
    "\n",
    "forgotten_memes = {}\n",
    "def forgotten_memes_per_degree(n_forgotten, followers):\n",
    "  if followers in forgotten_memes:\n",
    "    forgotten_memes[followers] += n_forgotten\n",
    "  else:\n",
    "    forgotten_memes[followers] = n_forgotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:25:07.550519Z",
     "start_time": "2020-05-11T16:25:07.546888Z"
    }
   },
   "outputs": [],
   "source": [
    "# track retweet memes\n",
    "# using a global variable that needs to be reset\n",
    "\n",
    "tracked_memes = {}\n",
    "def track_memes(meme):\n",
    "  if meme in tracked_memes:\n",
    "    tracked_memes[meme] += 1\n",
    "  else:\n",
    "    tracked_memes[meme] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T18:19:53.674880Z",
     "start_time": "2020-05-11T18:19:53.671236Z"
    }
   },
   "outputs": [],
   "source": [
    "# count bad meme selected times\n",
    "# using a global variable that needs to be reset\n",
    "\n",
    "bad_memes_seleted_time = defaultdict(lambda :[0, 0]) # {\"meme\": [human_node_select, bot_node_select]}\n",
    "def select_time(meme, bot_flag):\n",
    "  if bot_flag:\n",
    "    bad_memes_seleted_time[meme][1] += 1\n",
    "  else:\n",
    "    bad_memes_seleted_time[meme][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T17:55:08.744927Z",
     "start_time": "2020-05-11T17:55:08.735322Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "i_6y-w-hvdjA"
   },
   "outputs": [],
   "source": [
    "# a single simulation step in which one agent is activated\n",
    "\n",
    "def simulation_step(G,\n",
    "                    count_forgotten_memes=False,\n",
    "                    track_retweet_meme=False,\n",
    "                    count_select_time=False):\n",
    "  agent = random.choice(list(G.nodes()))\n",
    "  memes_in_feed = G.nodes[agent]['feed']\n",
    "  if len(memes_in_feed) and random.random() > mu:\n",
    "    # retweet a meme from feed selected on basis of its fitness\n",
    "    fitnesses = [m[1] for m in memes_in_feed]\n",
    "    meme = random.choices(memes_in_feed, weights=fitnesses, k=1)[0]\n",
    "  else:\n",
    "    # new meme\n",
    "    meme = get_meme(G.nodes[agent]['bot'])\n",
    "  \n",
    "  if track_retweet_meme:\n",
    "    track_memes(meme)\n",
    "  \n",
    "  if count_select_time and meme[0] == 0:\n",
    "    select_time(meme, G.nodes[agent]['bot'])\n",
    "\n",
    "  # spread (truncate feeds at max len alpha)\n",
    "  followers = G.predecessors(agent)\n",
    "  for f in followers:\n",
    "    #print('follower feed before:', [\"{0:.2f}\".format(round(m[0], 2)) for m in G.nodes[f]['feed']])   \n",
    "    G.nodes[f]['feed'].insert(0, meme)\n",
    "    if len(G.nodes[f]['feed']) > alpha:\n",
    "      if count_forgotten_memes and G.nodes[f]['bot'] == False:\n",
    "        # count only forgotten memes with zero quality\n",
    "        forgotten_zeros = 0\n",
    "        for m in G.nodes[f]['feed'][alpha:]:\n",
    "          if m[0] == 0:\n",
    "            forgotten_zeros += 1\n",
    "        forgotten_memes_per_degree(forgotten_zeros, G.in_degree(f))\n",
    "      del G.nodes[f]['feed'][alpha:]\n",
    "      #print('follower feed after :', [\"{0:.2f}\".format(round(m[0], 2)) for m in G.nodes[f]['feed']]) \n",
    "  #print('Bot' if G.nodes[agent]['bot'] else 'Human', 'posted', meme, 'to', G.in_degree(agent), 'followers', flush=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T16:25:13.985681Z",
     "start_time": "2020-05-11T16:25:13.981593Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GstjXKBf8nV3"
   },
   "outputs": [],
   "source": [
    "# calculate average quality of memes in system\n",
    "\n",
    "def measure_average_quality(G, count_bot=False):\n",
    "  total = 0\n",
    "  count = 0\n",
    "  for agent in G.nodes:\n",
    "    if count_bot == True or G.nodes[agent]['bot'] == False:\n",
    "      for m in G.nodes[agent]['feed']:\n",
    "        count += 1\n",
    "        total += m[0]\n",
    "  return total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T14:23:23.963159Z",
     "start_time": "2020-05-11T14:23:23.959001Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "H_xh7hNNeeeR"
   },
   "outputs": [],
   "source": [
    "# calculate fraction of low-quality memes in system\n",
    "\n",
    "def measure_average_zero_fraction(G):\n",
    "  count = 0\n",
    "  zeros = 0 \n",
    "  for agent in G.nodes:\n",
    "    if G.nodes[agent]['bot'] == False:\n",
    "      for m in G.nodes[agent]['feed']:\n",
    "        count += 1\n",
    "        if m[0] == 0: \n",
    "          zeros += 1 \n",
    "  return zeros / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T14:23:23.971411Z",
     "start_time": "2020-05-11T14:23:23.965467Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "48L9wu9JI56b"
   },
   "outputs": [],
   "source": [
    "# new network from old but replace feed with average quality\n",
    "\n",
    "def add_avq_to_net(G):\n",
    "  newG = G.copy()\n",
    "  for agent in newG.nodes:\n",
    "    if len(newG.nodes[agent]['feed']) < 1:\n",
    "      print('Bot' if newG.nodes[agent]['bot'] else 'Human', 'has empty feed')\n",
    "    newG.nodes[agent]['feed'] = float(statistics.mean([m[0] for m in G.nodes[agent]['feed']]))\n",
    "  return newG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T17:55:36.339896Z",
     "start_time": "2020-05-11T17:55:36.334326Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sZ31ZLajovII"
   },
   "outputs": [],
   "source": [
    "# main simulation \n",
    "# steady state is determined by small relative change in average quality\n",
    "# returns average quality at steady state \n",
    "\n",
    "def simulation(preferential_targeting_flag, return_net=False,\n",
    "               count_forgotten=False,\n",
    "               track_meme=False,\n",
    "               count_select=False):\n",
    "  network = init_net(preferential_targeting_flag)\n",
    "  n_agents = nx.number_of_nodes(network)\n",
    "  old_quality = 100\n",
    "  new_quality = 200\n",
    "  time_steps = 0\n",
    "  while max(old_quality, new_quality) > 0 and abs(new_quality - old_quality) / max(old_quality, new_quality) > epsilon: \n",
    "    #print('time_steps = ', time_steps, ', q = ', new_quality) \n",
    "    time_steps += 1\n",
    "    for _ in range(n_agents):\n",
    "      simulation_step(network,\n",
    "                      count_forgotten_memes=count_forgotten,\n",
    "                      track_retweet_meme=track_meme,\n",
    "                      count_select_time=count_select)\n",
    "  \n",
    "    old_quality = new_quality\n",
    "    new_quality = measure_average_quality(network)\n",
    "  if return_net:\n",
    "    return (new_quality, network)\n",
    "  else:\n",
    "    return new_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T14:23:23.987261Z",
     "start_time": "2020-05-11T14:23:23.982023Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "p6zwCs9nPj4m"
   },
   "outputs": [],
   "source": [
    "# append to file, locking file and waiting if busy in case of multi-processing\n",
    "\n",
    "def save_csv(data_array): \n",
    "  with open(cvsfile, 'a', newline='') as file:\n",
    "    while True:\n",
    "      try:\n",
    "        fcntl.flock(file, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(data_array)\n",
    "        fcntl.flock(file, fcntl.LOCK_UN)\n",
    "        break\n",
    "      except:\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T14:23:27.279471Z",
     "start_time": "2020-05-11T14:23:27.273685Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ueow4oT2tr3e"
   },
   "outputs": [],
   "source": [
    "# read from file\n",
    "\n",
    "def read_csv(filename):\n",
    "  q_mean_random = {} \n",
    "  q_stderr_random = {} \n",
    "  q_mean_preferential = {} \n",
    "  q_stderr_preferential = {} \n",
    "  q_mean_ratio = {} \n",
    "  q_stderr_ratio = {} \n",
    "  with open(filename, newline='') as file:\n",
    "    reader = csv.reader(file, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for row in reader:\n",
    "      q_mean_random[row[0]] = row[1]\n",
    "      q_stderr_random[row[0]] = row[2]\n",
    "      q_mean_preferential[row[0]] = row[3]\n",
    "      q_stderr_preferential[row[0]] = row[4]\n",
    "      q_mean_ratio[row[0]] = row[5]\n",
    "      q_stderr_ratio[row[0]] = row[6]\n",
    "  return(q_mean_random, q_stderr_random, \n",
    "         q_mean_preferential, q_stderr_preferential, \n",
    "         q_mean_ratio, q_stderr_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:22:53.798754Z",
     "start_time": "2020-05-25T15:22:53.790234Z"
    }
   },
   "outputs": [],
   "source": [
    "base = 1.5\n",
    "def logbase(x):\n",
    "    return np.log(x)/np.log(base)\n",
    "\n",
    "def get_count(list):\n",
    "    count = {}\n",
    "    for i, q in enumerate(list):\n",
    "        if q in count.keys():\n",
    "            count[q]+=1\n",
    "        else:\n",
    "            count[q]=1\n",
    "    return count\n",
    "\n",
    "def get_distr(count):\n",
    "    distr = {}\n",
    "    sum = 0\n",
    "    aver = 0\n",
    "    for a in count.keys():\n",
    "        sum += count[a]\n",
    "        aver += a*count[a]\n",
    "        bin = int(logbase(a))\n",
    "        if bin in distr.keys():\n",
    "            distr[bin] += count[a]\n",
    "        else:\n",
    "            distr[bin] = count[a]\n",
    "    return distr, sum\n",
    "\n",
    "def getbins(distr, sum):\n",
    "    mids = []\n",
    "    heights = []\n",
    "    bin = sorted(distr.keys())\n",
    "    for i in bin:\n",
    "        start = base ** i\n",
    "        width = base ** (i+1)-start\n",
    "        mid = start + width/2\n",
    "        mids.append(mid)\n",
    "        heights.append(distr[i]/(sum * width))\n",
    "    return mids, heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:22:55.031788Z",
     "start_time": "2020-05-25T15:22:55.022976Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_heatmap(ax, data, xticks, yticks, xlabel, ylabel, cmap, title, vmax=None, vmin=None):\n",
    "    data = data[::-1, :]\n",
    "    if vmin == None:\n",
    "        vmin = data[0][0]\n",
    "        for i in data:\n",
    "            for j in i:\n",
    "                if j<vmin:\n",
    "                    vmin=j\n",
    "    if vmax == None:\n",
    "        vmax = data[0][0]\n",
    "        for i in data:\n",
    "            for j in i:\n",
    "                if j>vmax:\n",
    "                    vmax=j\n",
    "\n",
    "    map = ax.imshow(data, interpolation='nearest', cmap=cmap, aspect='auto', vmin=vmin, vmax=vmax)\n",
    "    yticks = yticks[::-1]\n",
    "    ax.set_yticks(range(len(yticks)))\n",
    "    ax.set_yticklabels(yticks, fontsize=14)\n",
    "    ax.set_xticks(range(len(xticks)))\n",
    "    ax.set_xticklabels(xticks, fontsize=14)#, rotation=40\n",
    "    cb = plt.colorbar(mappable=map, cax=None, ax=None)\n",
    "    cb.ax.tick_params(labelsize=12)\n",
    "    plt.xlabel(xlabel, fontsize=14)\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqvOoIzuIJGY"
   },
   "source": [
    "Above are definitions\n",
    "\n",
    "---\n",
    "\n",
    "Below is main experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T15:49:24.918797Z",
     "start_time": "2020-05-14T15:41:12.696429Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qDd3WKkdndkK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment, save results to CSV file\n",
    "# this is slow for large n_humans; better to run in parallel \n",
    "# on a server or cluster, eg, one process per gamma value\n",
    "save_dir = \"results/random\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "q_random_all = {}\n",
    "for phi in phis:\n",
    "  for gamma in gammas:\n",
    "    q_random = []\n",
    "    valid_tracked_memes_random_all = []\n",
    "    bad_memes_selected_time_random_all = {}\n",
    "    avg_quality_random_all = []\n",
    "    avg_diversity_random_all = []\n",
    "    for sim in range(n_runs):\n",
    "      print('Running Simulation ', sim, ' for phi = ', phi, ', gamma = ', gamma, ' ...', flush=True)\n",
    "    \n",
    "      # reset global variable\n",
    "      global forgotten_memes, tracked_memes, bad_memes_seleted_time\n",
    "      assert(forgotten_memes != None)\n",
    "      assert(tracked_memes != None)\n",
    "      assert(bad_memes_seleted_time != None)\n",
    "      forgotten_memes = {}\n",
    "      tracked_memes = {}\n",
    "      bad_memes_seleted_time = defaultdict(lambda :[0, 0])\n",
    "\n",
    "      # simulation start\n",
    "      qr, qr_net = simulation(False, True, True, True, True) # random attach\n",
    "      q_random.append(qr)\n",
    "      if (phi, gamma) not in q_random_all:\n",
    "        q_random_all[(phi, gamma)] = []\n",
    "      q_random_all[(phi, gamma)].append(qr)\n",
    "    \n",
    "      #### statistic current nth-run data ####\n",
    "      ## tracked meme ##\n",
    "      valid_tracked_memes = []\n",
    "      for meme in tracked_memes:\n",
    "        valid = True\n",
    "        for agent in qr_net.nodes:\n",
    "          for m in qr_net.nodes[agent]['feed']:\n",
    "            if meme == m:\n",
    "              valid = False\n",
    "        if valid:\n",
    "          valid_tracked_memes.append((meme[0], tracked_memes[meme]))\n",
    "      valid_tracked_memes_random_all.extend(valid_tracked_memes)\n",
    "      ## end tracked meme ##\n",
    "    \n",
    "      ## bad meme select ##\n",
    "      for meme, selected_time in bad_memes_seleted_time.items():\n",
    "        if meme[1] not in bad_memes_selected_time_all:\n",
    "          bad_memes_selected_time_random_all[meme[1]] = [0, 0]\n",
    "        bad_memes_selected_time_random_all[meme[1]][0] += selected_time[0]\n",
    "        bad_memes_selected_time_random_all[meme[1]][1] += selected_time[1]\n",
    "      ## end bad meme select ##\n",
    "\n",
    "      ## avg quality ##\n",
    "      avg_quality_random_all.append(qr)\n",
    "      ## end avg quality ##\n",
    "\n",
    "      ## avg diversity ##\n",
    "      for agent in qr_net.nodes:\n",
    "        qualities = []\n",
    "        fitnesses = []\n",
    "        for m in qr_net.nodes[agent]['feed']:\n",
    "          qualities.append(m[0])\n",
    "          fitnesses.append(m[1])\n",
    "        unique_qua, unique_qua_cnt = np.unique(qualities, return_counts=True)\n",
    "        portion_of_qua = unique_qua_cnt / np.sum(unique_qua_cnt)\n",
    "        diversity = - np.sum(portion_of_qua * np.log(portion_of_qua))\n",
    "        avg_diversity_random_all.append(diversity)\n",
    "        \n",
    "        # unique_fit, unique_fit_cnt = np.unique(fitnesses, return_counts=True)\n",
    "        # portion_of_fit = unique_fit_cnt / np.sum(unique_fit_cnt)\n",
    "        # diversity = - np.sum(portion_of_fit * np.log(portion_of_fit))\n",
    "        # avg_diversity_random_all.append(diversity)\n",
    "      ## end avg diversity ##\n",
    "      #### end statistic current nth-run data ####\n",
    "\n",
    "    for fitness, selected_time in bad_memes_selected_time_all.items():\n",
    "      bad_memes_selected_time_all[fitness][0] /= n_runs\n",
    "      bad_memes_selected_time_all[fitness][1] /= n_runs\n",
    "\n",
    "    # save tracked memes\n",
    "    fp = open(\"{}/tracked_memes_random_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(valid_tracked_memes_random_all, fp)\n",
    "    fp.close()\n",
    "\n",
    "    # save bad meme selected times\n",
    "    fp = open(\"{}/bad_memes_selected_time_random_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(bad_memes_selected_time_random_all, fp)\n",
    "    fp.close()\n",
    "\n",
    "    # save avg_quality\n",
    "    fp = open(\"{}/avg_quality_random_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(np.mean(avg_quality_random_all), fp)\n",
    "    fp.close()\n",
    "    \n",
    "    # save avg_fitness\n",
    "    fp = open(\"{}/avg_diversity_random_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(np.mean(avg_diversity_random_all), fp)\n",
    "    fp.close()\n",
    "\n",
    "    # save kendall\n",
    "    quality, number_selected = zip(*valid_tracked_memes_random_all)\n",
    "    kendall_tau, _ = stats.kendalltau(quality, number_selected)\n",
    "    fp = open(\"{}/kendall_random_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(kendall_tau, fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T15:58:08.398057Z",
     "start_time": "2020-05-14T15:49:25.397948Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_dir = \"results/prefer\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "q_prefer_all = {}\n",
    "for phi in phis:\n",
    "  for gamma in gammas:\n",
    "    q_prefer = []\n",
    "    valid_tracked_memes_prefer_all = []\n",
    "    bad_memes_selected_time_prefer_all = {}\n",
    "    avg_quality_prefer_all = []\n",
    "    avg_diversity_prefer_all = []\n",
    "    for sim in range(n_runs):\n",
    "      print('Running Simulation ', sim, ' for phi = ', phi, ', gamma = ', gamma, ' ...', flush=True)\n",
    "\n",
    "      # reset global variable\n",
    "      global forgotten_memes, tracked_memes, bad_memes_seleted_time\n",
    "      assert(forgotten_memes != None)\n",
    "      assert(tracked_memes != None)\n",
    "      assert(bad_memes_seleted_time != None)\n",
    "      forgotten_memes = {}\n",
    "      tracked_memes = {}\n",
    "      bad_memes_seleted_time = defaultdict(lambda :[0, 0])\n",
    "\n",
    "      # simulation start\n",
    "      qp, qp_net = simulation(True, True, True, True, True) # preferential attach\n",
    "      q_prefer.append(qp)\n",
    "      if (phi, gamma) not in q_prefer_all:\n",
    "        q_prefer_all[(phi, gamma)] = []\n",
    "      q_prefer_all[(phi, gamma)].append(qp)\n",
    "    \n",
    "      #### statistic current nth-run data ####\n",
    "      ## tracked meme ##\n",
    "      valid_tracked_memes = []\n",
    "      for meme in tracked_memes:\n",
    "        valid = True\n",
    "        for agent in qr_net.nodes:\n",
    "          for m in qr_net.nodes[agent]['feed']:\n",
    "            if meme == m:\n",
    "              valid = False\n",
    "        if valid:\n",
    "          valid_tracked_memes.append((meme[0], tracked_memes[meme]))\n",
    "      valid_tracked_memes_prefer_all.extend(valid_tracked_memes)\n",
    "      ## end tracked meme ##\n",
    "    \n",
    "      ## bad meme select ##\n",
    "      for meme, selected_time in bad_memes_seleted_time.items():\n",
    "        if meme[1] not in bad_memes_selected_time_all:\n",
    "          bad_memes_selected_time_prefer_all[meme[1]] = [0, 0]\n",
    "        bad_memes_selected_time_prefer_all[meme[1]][0] += selected_time[0]\n",
    "        bad_memes_selected_time_prefer_all[meme[1]][1] += selected_time[1]\n",
    "      ## end bad meme select ##\n",
    "\n",
    "      ## avg quality ##\n",
    "      avg_quality_prefer_all.append(qp)\n",
    "      ## end avg quality ##\n",
    "\n",
    "      ## avg diversity ##\n",
    "      for agent in qp_net.nodes:\n",
    "        qualities = []\n",
    "        fitnesses = []\n",
    "        for m in qp_net.nodes[agent]['feed']:\n",
    "          qualities.append(m[0])\n",
    "          fitnesses.append(m[1])\n",
    "        unique_qua, unique_qua_cnt = np.unique(qualities, return_counts=True)\n",
    "        portion_of_qua = unique_qua_cnt / np.sum(unique_qua_cnt)\n",
    "        diversity = - np.sum(portion_of_qua * np.log(portion_of_qua))\n",
    "        avg_diversity_prefer_all.append(diversity)\n",
    "        \n",
    "        # unique_fit, unique_fit_cnt = np.unique(fitnesses, return_counts=True)\n",
    "        # portion_of_fit = unique_fit_cnt / np.sum(unique_fit_cnt)\n",
    "        # diversity = - np.sum(portion_of_fit * np.log(portion_of_fit))\n",
    "        # avg_diversity_prefer_all.append(diversity)\n",
    "      ## end avg diversity ##\n",
    "      #### end statistic current nth-run data ####\n",
    "\n",
    "    for fitness, selected_time in bad_memes_selected_time_all.items():\n",
    "      bad_memes_selected_time_all[fitness][0] /= n_runs\n",
    "      bad_memes_selected_time_all[fitness][1] /= n_runs\n",
    "\n",
    "    # save tracked memes\n",
    "    fp = open(\"{}/tracked_memes_prefer_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(valid_tracked_memes_prefer_all, fp)\n",
    "    fp.close()\n",
    "\n",
    "    # save bad meme selected times\n",
    "    fp = open(\"{}/bad_memes_selected_time_prefer_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(bad_memes_selected_time_prefer_all, fp)\n",
    "    fp.close()\n",
    "\n",
    "    # save avg_quality\n",
    "    fp = open(\"{}/avg_quality_prefer_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(np.mean(avg_quality_prefer_all), fp)\n",
    "    fp.close()\n",
    "    \n",
    "    # save avg_fitness\n",
    "    fp = open(\"{}/avg_diversity_prefer_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(np.mean(avg_diversity_prefer_all), fp)\n",
    "    fp.close()\n",
    "\n",
    "    # save kendall\n",
    "    quality, number_selected = zip(*valid_tracked_memes_prefer_all)\n",
    "    kendall_tau, _ = stats.kendalltau(quality, number_selected)\n",
    "    fp = open(\"{}/kendall_prefer_phi{}_gamma{}.pkl\".format(save_dir, phi, gamma), \"wb\")\n",
    "    pickle.dump(kendall_tau, fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T15:58:13.284621Z",
     "start_time": "2020-05-14T15:58:13.269804Z"
    }
   },
   "outputs": [],
   "source": [
    "q_ratio = []\n",
    "\n",
    "for phi in phis:\n",
    "  for gamma in gammas:\n",
    "    q_random = q_random_all[(phi, gamma)]\n",
    "    q_preferential = q_prefer_all[(phi, gamma)]\n",
    "    q_ratio = (np.array(q_preferential) / np.array(q_random)).tolist()\n",
    "    # save results to CSV file\n",
    "    save_csv([gamma, statistics.mean(q_random), \n",
    "            statistics.stdev(q_random) / math.sqrt(n_runs), \n",
    "            statistics.mean(q_preferential), \n",
    "            statistics.stdev(q_preferential) / math.sqrt(n_runs), \n",
    "            statistics.mean(q_ratio), \n",
    "            statistics.stdev(q_ratio) / math.sqrt(n_runs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T15:58:16.564851Z",
     "start_time": "2020-05-14T15:58:16.031528Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "atc5bUxE0Trf"
   },
   "outputs": [],
   "source": [
    "# plot data from CSV file\n",
    "\n",
    "q_mean_random, q_stderr_random, q_mean_preferential, q_stderr_preferential, q_mean_ratio, q_stderr_ratio = read_csv(cvsfile)\n",
    "\n",
    "ymin = [q_mean_ratio[x] - q_stderr_ratio[x] for x in q_mean_ratio.keys()]\n",
    "ymax = [q_mean_ratio[x] + q_stderr_ratio[x] for x in q_mean_ratio.keys()]\n",
    "plt.xlabel(r'$\\gamma$', fontsize=16)\n",
    "plt.ylabel('Average Quality Ratio', fontsize=16)\n",
    "plt.xscale('log')\n",
    "plt.axhline(y=1, lw=0.5, color='black')\n",
    "plt.plot(list(q_mean_ratio.keys()), list(q_mean_ratio.values()))\n",
    "plt.fill_between(list(q_mean_ratio.keys()), ymax, ymin, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T15:58:21.623637Z",
     "start_time": "2020-05-14T15:58:20.816659Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "-NjCcdb424_q",
    "outputId": "80c1806e-9636-4f30-a04a-1f00d0626bb0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot from files for different values of mu\n",
    "\n",
    "plt.subplots(figsize=plt.figaspect(1.5))\n",
    "\n",
    "_, _, _, _, ratio_mu75, stderr_mu75 = read_csv('results.csv')\n",
    "ymin_mu75 = [ratio_mu75[x] - 2*stderr_mu75[x] for x in ratio_mu75.keys()]\n",
    "ymax_mu75 = [ratio_mu75[x] + 2*stderr_mu75[x] for x in ratio_mu75.keys()]\n",
    "plt.plot(list(ratio_mu75.keys()), list(ratio_mu75.values()), label=r'$\\mu=0.75$')\n",
    "plt.fill_between(list(ratio_mu75.keys()), ymax_mu75, ymin_mu75, alpha=0.2)\n",
    "\n",
    "_, _, _, _, ratio_mu25, stderr_mu25 = read_csv('results.csv')\n",
    "ymin_mu25 = [ratio_mu25[x] - 2*stderr_mu25[x] for x in ratio_mu25.keys()]\n",
    "ymax_mu25 = [ratio_mu25[x] + 2*stderr_mu25[x] for x in ratio_mu25.keys()]\n",
    "plt.plot(list(ratio_mu25.keys()), list(ratio_mu25.values()), label=r'$\\mu=0.25$')\n",
    "plt.fill_between(list(ratio_mu25.keys()), ymax_mu25, ymin_mu25, alpha=0.2)\n",
    "\n",
    "plt.xlabel(r'$\\gamma$', fontsize=16)\n",
    "plt.ylabel('Average Quality Ratio', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xscale('log')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.axhline(y=1, lw=0.5, color='black')\n",
    "plt.legend(fontsize=14, loc='upper center')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig_targeting.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDTP02u76ZmW"
   },
   "source": [
    "Above is main experiment\n",
    "\n",
    "---\n",
    "\n",
    "Everything below is supplementary testing and analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:25:56.755433Z",
     "start_time": "2020-05-25T15:25:54.671993Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot Fig3\n",
    "\n",
    "xs = phis\n",
    "ys = gammas\n",
    "phis1 = phis\n",
    "phis2 = phis\n",
    "wires = gammas\n",
    "new_wires = gammas\n",
    "cmap = None\n",
    "xlabel = '$\\\\phi$'\n",
    "ylabel = '$\\\\gamma$'\n",
    "\n",
    "kendall_pic_title = 'Discriminative power'\n",
    "avg_quality_pic_title = 'Average Quality'\n",
    "diversity_pic_title = 'Diversity'\n",
    "\n",
    "figure = plt.figure(figsize=(13, 15), facecolor='w')\n",
    "markers = [\"o\", \"s\", \"^\"]\n",
    "\n",
    "save_dir = \"results/prefer\"\n",
    "\n",
    "### 1. average quality ###\n",
    "if save_dir == \"results/random\":\n",
    "    file_template = \"{}/avg_quality_random_phi{}_gamma{}.pkl\"\n",
    "else:\n",
    "    file_template = \"{}/avg_quality_prefer_phi{}_gamma{}.pkl\"\n",
    "\n",
    "# distr plot\n",
    "ax = figure.add_subplot(3,2,1)\n",
    "for idx, phi in enumerate(phis1):\n",
    "    avg_qualities = []\n",
    "    stds = []\n",
    "    for gamma in wires:\n",
    "        fname = file_template.format(save_dir, phi, gamma)\n",
    "        fp = open(fname, \"rb\")\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "        avg_qualities.append(np.mean(data))\n",
    "        stds.append(np.std(data))\n",
    "    ax.plot(new_wires, avg_qualities, marker=markers[idx], label='$\\\\phi$:'+str(h))\n",
    "\n",
    "ax.set_xlabel('$\\\\gamma$', fontsize=14)\n",
    "ax.set_ylabel('Average quality', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim((new_wires[0], new_wires[-1]))\n",
    "ax.set_xlim((new_wires[0], new_wires[-1]))\n",
    "ax.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "# heatmap plot\n",
    "ax = figure.add_subplot(3,2,2)\n",
    "grid = np.zeros((len(wires), len(phis2)))\n",
    "for i, gamma in enumerate(wires):\n",
    "    for j, phi in enumerate(phis2):\n",
    "        fname = file_template.format(save_dir, phi, gamma)\n",
    "        fp = open(fname, \"rb\")\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "        grid[i, j] = np.mean(data)\n",
    "draw_heatmap(ax, grid, xs, ys, xlabel, ylabel, cmap, avg_quality_pic_title, vmin=None, vmax=None)\n",
    "\n",
    "\n",
    "### 2. average diversity ###\n",
    "if save_dir == \"results/random\":\n",
    "    file_template = \"{}/avg_diversity_prefer_phi{}_gamma{}.pkl\"\n",
    "else:\n",
    "    file_template = \"{}/avg_diversity_prefer_phi{}_gamma{}.pkl\"\n",
    "\n",
    "# distr plot\n",
    "ax = figure.add_subplot(3,2,3)\n",
    "for idx, phi in enumerate(phis1):\n",
    "    avg_diversities = []\n",
    "    stds = []\n",
    "    for gamma in wires:\n",
    "        fname = file_template.format(save_dir, phi, gamma)\n",
    "        fp = open(fname, \"rb\")\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "        avg_diversities.append(np.mean(data))\n",
    "        stds.append(np.std(data))\n",
    "    ax.plot(new_wires, avg_diversities, marker=markers[idx], label='$\\\\phi$:'+str(h))\n",
    "\n",
    "ax.set_xlabel('$\\\\gamma$', fontsize=14)\n",
    "ax.set_ylabel('Diversity', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim((new_wires[0], new_wires[-1]))\n",
    "ax.set_xlim((new_wires[0], new_wires[-1]))\n",
    "\n",
    "# heatmap plot\n",
    "ax = figure.add_subplot(3,2,4)\n",
    "grid = np.zeros((len(wires), len(phis2)))\n",
    "for i, gamma in enumerate(wires):\n",
    "    for j, phi in enumerate(phis2):\n",
    "        fname = file_template.format(save_dir, phi, gamma)\n",
    "        fp = open(fname, \"rb\")\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "        grid[i, j] = np.mean(data)\n",
    "draw_heatmap(ax, grid, xs, ys, xlabel, ylabel, cmap, diversity_pic_title, vmin=None, vmax=None)\n",
    "\n",
    "### 3. kendall ###\n",
    "if save_dir == \"results/random\":\n",
    "    file_template = \"{}/kendall_random_phi{}_gamma{}.pkl\"\n",
    "else:\n",
    "    file_template = \"{}/kendall_prefer_phi{}_gamma{}.pkl\"\n",
    "\n",
    "# distr plot\n",
    "ax = figure.add_subplot(3,2,5)\n",
    "for idx, phi in enumerate(phis1):\n",
    "    kendalls = []\n",
    "    stds = []\n",
    "    for gamma in wires:\n",
    "        fname = file_template.format(save_dir, phi, gamma)\n",
    "        fp = open(fname, \"rb\")\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "        kendalls.append(np.mean(data))\n",
    "        stds.append(np.std(data))\n",
    "    ax.plot(new_wires, kendalls, marker=markers[idx], label='$\\\\phi$:'+str(h))\n",
    "\n",
    "ax.set_xlabel('$\\\\gamma$', fontsize=14)\n",
    "ax.set_ylabel('Discriminative power', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim((new_wires[0], new_wires[-1]))\n",
    "ax.set_xlim((new_wires[0], new_wires[-1]))\n",
    "# ax.legend(loc='lower left', fontsize=14)\n",
    "\n",
    "# heatmap plot\n",
    "ax = figure.add_subplot(3,2,6)\n",
    "grid = np.zeros((len(wires), len(phis2)))\n",
    "for i, gamma in enumerate(wires):\n",
    "    for j, phi in enumerate(phis2):\n",
    "        fname = file_template.format(save_dir, phi, gamma)\n",
    "        fp = open(fname, \"rb\")\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "        grid[i, j] = np.mean(data)\n",
    "draw_heatmap(ax, grid, xs, ys, xlabel, ylabel, cmap, kendall_pic_title, vmin=None, vmax=None)\n",
    "\n",
    "### 4. save plot ###\n",
    "plt.subplots_adjust(left=0.1, right=0.95, top=0.95, bottom=0.05, wspace=0.3, hspace=0.3)\n",
    "plt.savefig(save_dir + \"/all_distr_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T16:02:49.966704Z",
     "start_time": "2020-05-14T16:02:46.542128Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot Fig5\n",
    "\n",
    "save_dir = \"results/random\"\n",
    "if save_dir == \"results/random\":\n",
    "    file_template = \"{}/tracked_memes_random_phi{}_gamma{}.pkl\"\n",
    "else:\n",
    "    file_template = \"{}/tracked_memes_prefer_phi{}_gamma{}.pkl\"\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(14, 8))\n",
    "for i, phi in enumerate([1, 10]):\n",
    "    for j, gamma in enumerate([0.001, 0.005, 0.01]):\n",
    "        fname = file_template.format(save_dir, phi, gamma)\n",
    "        fp = open(fname, \"rb\")\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "\n",
    "        quality, number_selected = zip(*data)\n",
    "\n",
    "        low_quality_pop = []\n",
    "        high_quality_pop = []\n",
    "        for qua, pop in zip(quality, number_selected):\n",
    "            if qua > 0:\n",
    "                high_quality_pop.append(pop)\n",
    "            else:\n",
    "                low_quality_pop.append(pop)\n",
    "\n",
    "        count = get_count(high_quality_pop)\n",
    "        distr, sum_ = get_distr(count)\n",
    "        h_mids, h_heights = getbins(distr, sum_)\n",
    "\n",
    "        count = get_count(low_quality_pop)\n",
    "        distr, sum_ = get_distr(count)\n",
    "        l_mids, l_heights = getbins(distr, sum_)\n",
    "\n",
    "        h_dict = defaultdict(list)\n",
    "        for hm, hh in zip(h_mids, h_heights):\n",
    "            h_dict[hm].append(hh)\n",
    "        l_dict = defaultdict(list)\n",
    "        for lm, lh in zip(l_mids, l_heights):\n",
    "            l_dict[lm].append(lh)\n",
    "\n",
    "        hs = []\n",
    "        for k, v in h_dict.items():\n",
    "            hs.append([k, np.mean(v)])\n",
    "        h_mids, h_heights = zip(*sorted(hs, key=lambda x:x[0]))\n",
    "        ls = []\n",
    "        for k, v in l_dict.items():\n",
    "            ls.append([k, np.mean(v)])\n",
    "        l_mids, l_heights = zip(*sorted(ls, key=lambda x:x[0]))\n",
    "\n",
    "        ax = axs[i][j]\n",
    "        ax.loglog(h_mids, h_heights, marker='s', label='high quality')\n",
    "        ax.loglog(l_mids, l_heights, marker='^', label='low quality')\n",
    "        ax.set_xlabel('popularity', fontsize=14)\n",
    "        ax.set_ylabel('P(popularity)', fontsize=14)\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.annotate('$\\\\gamma={}$\\n$\\\\phi={}$'.format(gamma, phi), xy=(0.05, 0.05), xycoords='axes fraction', fontsize=12)\n",
    "        if i == 0 and j == 0:\n",
    "            ax.legend(loc=\"upper right\", fontsize=15)\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.92, top=0.92, wspace=0.3, hspace=0.3)\n",
    "plt.show()\n",
    "plt.savefig(save_dir + \"meme_quality_random_distr.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:26:19.203080Z",
     "start_time": "2020-05-25T15:26:18.292723Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot Fig6\n",
    "\n",
    "save_dir = \"results/random\"\n",
    "if save_dir == \"results/random\":\n",
    "    file_template = \"{}/bad_memes_selected_time_random_phi{}_gamma{}.pkl\"\n",
    "else:\n",
    "    file_template = \"{}/bad_memes_selected_time_prefer_phi{}_gamma{}.pkl\"\n",
    "\n",
    "for i, phi in enumerate([1]):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for j, gamma in enumerate([0.001]): #[0.5]\n",
    "        fname = file_template.format(save_dir, phi, gamma)\n",
    "        fp = open(fname, \"rb\")\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "        \n",
    "        good_selected = []\n",
    "        bad_selected = []\n",
    "        for _, value in data.items():\n",
    "            if value[0] <= 0 or value[1] <= 0:\n",
    "                continue\n",
    "            good_selected.append(value[0])\n",
    "            bad_selected.append(value[1])\n",
    "\n",
    "        count = dict([val for val in zip(bad_selected, good_selected)])\n",
    "        distr_x, distr_y = get_distr(count)\n",
    "        mids, heights = getbins(distr_x, distr_y)\n",
    "        ratios = [np.log(height_)/np.log(mid_) for height_, mid_ in zip(heights, mids)]\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.loglog(mids, heights, marker='o', label='$\\\\gamma$:'+str(gamma))\n",
    "        plt.subplot(122)\n",
    "        plt.plot(mids, ratios, marker='o', label='$\\\\gamma$:'+str(gamma))\n",
    "        plt.xscale('log')\n",
    "    \n",
    "    # save fig\n",
    "    plt.subplot(121)\n",
    "    plt.loglog([min(mids), max(mids)], [min(mids), max(mids)], '--')\n",
    "    plt.xlabel(\"Bot posts per meme\", fontsize=14)\n",
    "    plt.ylabel(\"Human posts per meme\", fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.margins(0.1)\n",
    "    plt.legend(loc='best', fontsize=14)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.xlabel(\"Bot posts per meme\", fontsize=14)\n",
    "    plt.ylabel(\"Exponent $\\\\eta$\", fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.margins(0.1)\n",
    "    plt.legend(loc='best', fontsize=14)\n",
    "\n",
    "    plt.subplots_adjust(left=0.1, bottom=0.14, wspace=0.4)\n",
    "    plt.show()\n",
    "    plt.savefig(save_dir + \"bad_meme_selected_random_distr_{}\".format(phi))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bot_model_xiaodan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
